{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Keras and MNIST.\n",
    "\n",
    "When it comes to Deep Learning there are many viable framework-options. One is Keras (https://keras.io), which we currently use most. Note that we do not restrict ourselves to Keras-only. Me might use other frameworks in the future. Nevertheless Keras is excellent when it comes to rapid prototyping. And it is way easier to understand than for example TensorFlow and PyTorch.\n",
    "\n",
    "MNIST (https://en.wikipedia.org/wiki/MNIST_database) is a database of handwritten digits. It poses a supervised learning problem. This means that you have labelled data. In our case pictures of the mentioned digits plus the actual digit as a number. MNIST is the Hello-World of Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports.\n",
    "\n",
    "Firstly, we import all the necessary modules. Keras comes with a wide variety of modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters.\n",
    "\n",
    "Deep Learning is to a certain degree a hyperparameter-optimization problem. In order to find the best model, you have to tweak those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data and preprocess it.\n",
    "\n",
    "In Deep Learning the most significant part is not training the Neural Network. It is getting and preprocessing data. In the following we will load the MNIST data. The inputs will be normalized, the outputs will be categorically encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Neural Network architecture.\n",
    "\n",
    "Now we will create a fully connected Neural Network with a couple of layers. The network in its essence takes an image of a handwritten digit and maps it to a symbolic representation of a digit. This symbolic representation is a probability distribution. This is a so called classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the model.\n",
    "\n",
    "Compiling means:\n",
    "- Adding a loss, which is a number that tells you how big the error of the current network is.\n",
    "- Adding an optimizer. That is a training algorithm that trains the network.\n",
    "- Adding one or multiple metrics (optional). Providing a means for humans to understand how good the network is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Neural Network.\n",
    "\n",
    "Training means fitting the network over time, that is over a number of epochs, using training data. This means changing weights and parameters of the network. Also it means validating the network against a validation set at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.2475 - acc: 0.9246 - val_loss: 0.1446 - val_acc: 0.9544\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.1025 - acc: 0.9688 - val_loss: 0.0751 - val_acc: 0.9766\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.0743 - acc: 0.9775 - val_loss: 0.0773 - val_acc: 0.9775\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0585 - acc: 0.9823 - val_loss: 0.0797 - val_acc: 0.9789\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.0507 - acc: 0.9853 - val_loss: 0.0817 - val_acc: 0.9791\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.0439 - acc: 0.9867 - val_loss: 0.0750 - val_acc: 0.9816\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.0386 - acc: 0.9886 - val_loss: 0.0694 - val_acc: 0.9816\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.0321 - acc: 0.9905 - val_loss: 0.0881 - val_acc: 0.9805\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 0.0317 - acc: 0.9911 - val_loss: 0.0975 - val_acc: 0.9815\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.0284 - acc: 0.9919 - val_loss: 0.0850 - val_acc: 0.9843\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.0289 - acc: 0.9920 - val_loss: 0.0804 - val_acc: 0.9834\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.0234 - acc: 0.9934 - val_loss: 0.0948 - val_acc: 0.9815\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.0246 - acc: 0.9928 - val_loss: 0.0974 - val_acc: 0.9830\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.0236 - acc: 0.9937 - val_loss: 0.0906 - val_acc: 0.9851\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.0230 - acc: 0.9939 - val_loss: 0.1112 - val_acc: 0.9829\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.0210 - acc: 0.9946 - val_loss: 0.1039 - val_acc: 0.9830\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0194 - acc: 0.9948 - val_loss: 0.1039 - val_acc: 0.9837\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.0213 - acc: 0.9945 - val_loss: 0.1094 - val_acc: 0.9834\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.0197 - acc: 0.9950 - val_loss: 0.1125 - val_acc: 0.9827\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0206 - acc: 0.9947 - val_loss: 0.1112 - val_acc: 0.9838\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.11122665319426873\n",
      "Test accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
