{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/datathon/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import dbutils\n",
    "from datetime import date\n",
    "from time import mktime\n",
    "import calendar\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from keras import models\n",
    "import progressbar\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from cgmcore import modelutils\n",
    "from cgmcore import utils\n",
    "\n",
    "main_connector = dbutils.connect_to_main_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the PCD data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag 2018-9 has 2063 PCDs.\n",
      "Tag 2018-10 has 1871 PCDs.\n",
      "Tag 2018-11 has 385 PCDs.\n",
      "Tag 2018-12 has 443 PCDs.\n",
      "Tag all-data has 4762 PCDs.\n",
      "\n",
      "5 megabatches to check.\n"
     ]
    }
   ],
   "source": [
    "# This is the main variable.\n",
    "pcd_megabatches = []\n",
    "\n",
    "# The table to query.\n",
    "data_table = \"pointcloud_data\"\n",
    "\n",
    "# The values of the measurents-table that should appear in the results.\n",
    "values = [\"height_cms\"]\n",
    "values = [\"measurements.\" + value for value in values]\n",
    "values = \", \".join(values)\n",
    "\n",
    "\n",
    "for year in [2018, 2019]:\n",
    "    for month in range(1, 13):\n",
    "        \n",
    "        # Getting the start.\n",
    "        start_day = 1\n",
    "        start_date = date(year, month, start_day)\n",
    "        start_timestamp = str(1000 * int(mktime(start_date.timetuple())))\n",
    "        \n",
    "        # Getting the end.\n",
    "        _, end_day = calendar.monthrange(year, month)\n",
    "        end_date = date(year, month, end_day)\n",
    "        end_timestamp = str(1000 * int(mktime(end_date.timetuple())))\n",
    "\n",
    "        # Compose an SQL-statement.\n",
    "        sql_statement = \"\"\n",
    "        sql_statement += \"SELECT {}.path, {}\".format(data_table, values)\n",
    "        sql_statement += \" FROM {}\".format(data_table)\n",
    "        sql_statement += \" INNER JOIN measurements ON {}.measurement_id=measurements.id\".format(data_table)\n",
    "        sql_statement += \" WHERE measurements.type=\\'manual\\'\"\n",
    "        sql_statement += \" AND measurements.timestamp >= {}\".format(start_timestamp)\n",
    "        sql_statement += \" AND measurements.timestamp <= {}\".format(end_timestamp)\n",
    "        sql_statement += \" AND timestamp <= {}\".format(end_timestamp)\n",
    "        results = main_connector.execute(sql_statement, fetch_all=True)\n",
    "        if len(results) != 0:\n",
    "            pcd_megabatches.append((f\"{year}-{month}\", results))\n",
    "\n",
    "all_results = []\n",
    "for _, results in pcd_megabatches:\n",
    "    all_results.extend(results)\n",
    "pcd_megabatches.append((\"all-data\", all_results))\n",
    "\n",
    "    \n",
    "for tag, results in pcd_megabatches:\n",
    "    print(\"Tag {} has {} PCDs.\".format(tag, len(results)))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"{} megabatches to check.\".format(len(pcd_megabatches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 models\n"
     ]
    }
   ],
   "source": [
    "# Getting all the model paths.\n",
    "models_root_path = \"/whhdata/models\"\n",
    "model_paths = sorted(glob.glob(os.path.join(models_root_path, \"*.h5\")))\n",
    "model_paths = [model_path for model_path in model_paths if model_path.endswith(\"-model-weights.h5\") == False]\n",
    "print(\"Found {} models\".format(len(model_paths)))\n",
    "\n",
    "# Load one spare point net model.\n",
    "point_net_model = modelutils.create_point_net((30000, 3), 1, hidden_sizes = [512, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating models against mega-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating/whhdata/models/20181026-0710-voxnet-model.h5...\n",
      "Skipped.\n",
      "Evaluating/whhdata/models/20181029-1312-voxnet-model.h5...\n",
      "Skipped.\n",
      "Evaluating/whhdata/models/20181029-1314-voxnet-model.h5...\n",
      "Skipped.\n",
      "Evaluating/whhdata/models/20181029-1801-pointnet-model.h5...\n",
      "Skipped.\n",
      "Evaluating/whhdata/models/20181031-1201-pointnet-model.h5...\n",
      "Skipped.\n",
      "Evaluating/whhdata/models/20181031-1636-pointnet-model.h5...\n",
      "Skipped.\n",
      "Evaluating/whhdata/models/20181031-2038-pointnet-model.h5...\n",
      "Skipped.\n",
      "Evaluating/whhdata/models/20181101-1444-pointnet-model.h5...\n",
      "Skipped.\n",
      "Evaluating/whhdata/models/20181101-1643-pointnet-model.h5...\n",
      "Skipped.\n",
      "Evaluating/whhdata/models/20181101-2145-pointnet-model.h5...\n",
      "Skipped.\n",
      "Evaluating/whhdata/models/20181104-0039-2dCNN-model.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "N/A% (0 of 2063) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3) (1,)\n",
      "2018-9: 2063 PCDs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98% (2027 of 2063) |################### | Elapsed Time: 0:15:30 ETA:   0:00:10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot reshape array of size 0 into shape (0,newaxis)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2063 of 2063) |####################| Elapsed Time: 0:15:46 Time:  0:15:46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2062/2062 [==============================] - 3s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "N/A% (0 of 1871) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-9 926.0488759696657 26.540957202041877\n",
      "2018-10: 1871 PCDs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8% (164 of 1871) |#                    | Elapsed Time: 0:01:14 ETA:   0:10:43"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong number of columns at line 25736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25% (479 of 1871) |#####                | Elapsed Time: 0:03:39 ETA:   0:11:33"
     ]
    }
   ],
   "source": [
    "#batch_size = 128\n",
    "\n",
    "model_load_errors = 0\n",
    "\n",
    "# Routine for loading models.\n",
    "def load_model(model_path):\n",
    "    # Try to load the model.\n",
    "    try:\n",
    "        model = models.load_model(model_path)\n",
    "    except ValueError:\n",
    "        # Error. Could be a problem with pointnet, which needs to be loaded via weights.\n",
    "        try:\n",
    "            model = point_net_model\n",
    "            model.load_weights(model_path.replace(\"-model.h5\", \"-model-weights.h5\"))\n",
    "        except Exception as e:\n",
    "            # All hope is lost.\n",
    "            print(e)\n",
    "            print(\"Error loading model.\")\n",
    "            return None\n",
    "    return model\n",
    " \n",
    "# Evaluating models.\n",
    "def evaluate_model(model):\n",
    "    evaluation_results = []\n",
    "    \n",
    "    input_shape = model.inputs[0].shape[1:]\n",
    "    model_type = \"\"\n",
    "    if len(input_shape) == 3 and input_shape == (32, 32, 32):\n",
    "        model_type =\"voxnet\"\n",
    "    elif len(input_shape) == 3 and input_shape[2] == 3:\n",
    "        model_type =\"rgb\"\n",
    "    elif len(input_shape) == 2 and input_shape == (30000, 3):\n",
    "        model_type =\"pointcloud\"\n",
    "    else:\n",
    "        print(\"Unexpected:\", input_shape)\n",
    "        \n",
    "    output_shape = model.outputs[0].shape[1:]\n",
    "    print(input_shape, output_shape)\n",
    "    \n",
    "    for tag, samples in pcd_megabatches:\n",
    "        print(\"{}: {} PCDs.\".format(tag, len(samples)))\n",
    "        \n",
    "        x_input = []\n",
    "        y_output = []\n",
    "        bar = progressbar.ProgressBar(max_value=len(samples))\n",
    "        for index, (path, target) in enumerate(samples):\n",
    "            bar.update(index)\n",
    "            try:\n",
    "                pcd_array = utils.load_pcd_as_ndarray(path)\n",
    "                rgb_map = utils.pointcloud_to_rgb_map(pcd_array, target_width=64, target_height=64, scale_factor=1.0, axis=\"vertical\")\n",
    "                x_input.append(rgb_map)\n",
    "                y_output.append(target)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        bar.finish()  \n",
    "        \n",
    "        x_input = np.array(x_input)\n",
    "        y_output = np.array(y_output)\n",
    "\n",
    "        mse, mae = model.evaluate(x_input, y_output)\n",
    "        print(tag, mse, mae)\n",
    "        evaluation_results.append((tag, mse, mae))\n",
    "    return evaluation_results\n",
    "        \n",
    "        \n",
    "# Loop through all models.\n",
    "for model_index, model_path in enumerate(model_paths):\n",
    "    print(f\"Evaluating{model_path}...\")\n",
    "    \n",
    "    # Skip voxnets and pointnets (debugging).\n",
    "    if \"voxnet\" in model_path or \"pointnet\" in model_path:\n",
    "        print(\"Skipped.\")\n",
    "        continue\n",
    "    \n",
    "    # Attempt to load the model.\n",
    "    model = load_model(model_path)\n",
    "    if model == None:\n",
    "        model_load_errors += 1\n",
    "        continue\n",
    "    \n",
    "    # Evaluate the model.\n",
    "    # TODO put this to a database.\n",
    "    evaluation_results = evaluate_model(model)\n",
    "    for tag, mse, mae in evaluation_results:\n",
    "        pring(f\"Results for {model_path}:\")\n",
    "        print(f\"{tag} mse: {mse} mae: {mae}\")\n",
    "\n",
    "    del model\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "print(f\"{model_load_errors}/{len(model_paths)} models could not be loaded due to errors.\")\n",
    "    #print(\"Evaluating model \\\"{}\\\"...\".format(model_path))\n",
    "    #for batch_index, index in enumerate(range(0, len(results), batch_size)):\n",
    "    #    print(batch_index, index, index + batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
